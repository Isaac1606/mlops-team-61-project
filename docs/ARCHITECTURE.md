# ğŸ—ï¸ Project Architecture

This document describes the architecture of the Bike Sharing Demand Prediction project, following MLOps best practices.

## ğŸ“ Project Structure

```
mlops-team-61-project/
â”œâ”€â”€ config/                      # Configuration files
â”‚   â”œâ”€â”€ config.yaml              # Central configuration (YAML)
â”‚   â””â”€â”€ paths_config.py          # Legacy path config (deprecated)
â”‚
â”œâ”€â”€ data/                        # Data directory (DVC tracked)
â”‚   â”œâ”€â”€ raw/                     # Raw data files
â”‚   â”œâ”€â”€ interim/                 # Intermediate processed data
â”‚   â””â”€â”€ processed/               # Final processed datasets
â”‚
â”œâ”€â”€ models/                      # Trained models (DVC tracked)
â”‚   â”œâ”€â”€ *.pkl                    # Saved models
â”‚   â””â”€â”€ *_feature_importance.csv # Feature importance reports
â”‚
â”œâ”€â”€ src/                         # Source code (Python package)
â”‚   â”œâ”€â”€ config/                  # Configuration management
â”‚   â”‚   â”œâ”€â”€ config_loader.py     # YAML config loader
â”‚   â”‚   â””â”€â”€ paths.py             # Path management
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                    # Data processing module
â”‚   â”‚   â”œâ”€â”€ data_loader.py       # Data loading utilities
â”‚   â”‚   â”œâ”€â”€ data_cleaner.py     # Data cleaning operations
â”‚   â”‚   â”œâ”€â”€ feature_engineering.py # Feature engineering
â”‚   â”‚   â”œâ”€â”€ data_splitter.py    # Temporal data splitting
â”‚   â”‚   â””â”€â”€ make_dataset.py     # Data processing pipeline script
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                  # Modeling module
â”‚   â”‚   â”œâ”€â”€ preprocessor.py     # Scikit-Learn preprocessor
â”‚   â”‚   â”œâ”€â”€ pipeline.py          # Scikit-Learn pipeline wrapper
â”‚   â”‚   â”œâ”€â”€ model_trainer.py    # Model training with MLflow
â”‚   â”‚   â”œâ”€â”€ model_evaluator.py  # Model evaluation utilities
â”‚   â”‚   â””â”€â”€ train_model.py      # Training pipeline script
â”‚   â”‚
â”‚   â””â”€â”€ tools/                   # Utility functions (placeholder)
â”‚
â”œâ”€â”€ notebooks/                   # Jupyter notebooks (exploratory)
â”‚   â”œâ”€â”€ notebook.ipynb           # EDA notebook
â”‚   â””â”€â”€ 02_modeling.ipynb       # Modeling notebook
â”‚
â”œâ”€â”€ docs/                        # Documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md         # This file
â”‚   â”œâ”€â”€ ML_Canvas.md            # Business requirements
â”‚   â””â”€â”€ EDA_Summary.md          # EDA findings
â”‚
â”œâ”€â”€ reports/                     # Generated reports
â”‚   â””â”€â”€ figures/                # Visualizations
â”‚
â”œâ”€â”€ mlruns/                      # MLflow tracking data (gitignored)
â”‚
â”œâ”€â”€ config.yaml                  # Main configuration file
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ environment.yml             # Conda environment file
â”œâ”€â”€ setup.py                    # Package setup script
â”œâ”€â”€ Makefile                    # Make commands for reproducibility
â”œâ”€â”€ .dvc/                       # DVC configuration
â””â”€â”€ README.md                   # Project README
```

## ğŸ”„ Data Flow

```
Raw Data (CSV)
    â†“
[DataLoader] â†’ Load raw data
    â†“
[DataCleaner] â†’ Clean data (types, nulls, outliers)
    â†“
[FeatureEngineer] â†’ Create features (lags, rolling, cyclical, interactions)
    â†“
[DataSplitter] â†’ Split temporally (train/val/test)
    â†“
[DataPreprocessor] â†’ Scale features (RobustScaler)
    â†“
[MLPipeline] â†’ Preprocess + Model (Scikit-Learn Pipeline)
    â†“
[ModelTrainer] â†’ Train with MLflow tracking
    â†“
[ModelEvaluator] â†’ Evaluate metrics
    â†“
Trained Models + MLflow Experiments
```

## ğŸ¯ Design Principles

### 1. **Single Responsibility Principle**
Each class has one clear purpose:
- `DataLoader`: Loading data from files
- `DataCleaner`: Cleaning operations
- `FeatureEngineer`: Feature creation
- `ModelTrainer`: Training logic
- `ModelEvaluator`: Evaluation metrics

### 2. **Dependency Injection**
Classes receive dependencies via constructor:
```python
config = ConfigLoader()
paths = ProjectPaths(config)
trainer = ModelTrainer(config, paths)
```

### 3. **Scikit-Learn Compatibility**
All transformers implement `fit()`/`transform()` pattern:
```python
preprocessor = DataPreprocessor()
preprocessor.fit(X_train)
X_train_scaled = preprocessor.transform(X_train)
```

### 4. **Configuration-Driven**
All parameters come from `config/config.yaml`:
- No hardcoded values
- Easy to experiment
- Version controlled

### 5. **Reproducibility**
- Fixed random seeds
- DVC for data versioning
- MLflow for experiment tracking
- Environment files for dependencies

## ğŸ”§ Core Components

### Configuration Management

**`ConfigLoader`** (`src/config/config_loader.py`)
- Loads YAML configuration
- Provides typed access to config values
- Centralizes all parameters

**`ProjectPaths`** (`src/config/paths.py`)
- Manages all file paths
- Creates directories as needed
- Follows Cookiecutter structure

### Data Processing

**`DataLoader`** (`src/data/data_loader.py`)
- Loads raw and processed data
- Validates file existence
- Handles errors gracefully

**`DataCleaner`** (`src/data/data_cleaner.py`)
- Converts data types
- Handles null values
- Removes problematic columns

**`FeatureEngineer`** (`src/data/feature_engineering.py`)
- Creates lag features
- Rolling statistics
- Cyclical encodings
- Interaction features
- Advanced features (volatility, momentum)

**`DataSplitter`** (`src/data/data_splitter.py`)
- Temporal splitting (respects time order)
- Configurable split ratios
- Prevents data leakage

### Modeling

**`DataPreprocessor`** (`src/models/preprocessor.py`)
- Scikit-Learn compatible transformer
- Handles scaling (StandardScaler, RobustScaler, MinMaxScaler)
- Excludes binary/categorical features from scaling

**`MLPipeline`** (`src/models/pipeline.py`)
- Wraps preprocessing + model in Scikit-Learn Pipeline
- Ensures consistent transformations
- Prevents data leakage

**`ModelTrainer`** (`src/models/model_trainer.py`)
- Creates models from config
- Trains with MLflow tracking
- Saves trained models
- Logs hyperparameters and metrics

**`ModelEvaluator`** (`src/models/model_evaluator.py`)
- Computes multiple metrics (MAE, RMSE, RÂ², MAPE)
- Compares against targets
- Generates evaluation reports
- Extracts feature importance

## ğŸ”¬ Scikit-Learn Pipeline Integration

The project uses **Scikit-Learn Pipelines** for end-to-end ML workflows:

```python
from src.models import MLPipeline
from sklearn.ensemble import RandomForestRegressor

# Create pipeline
pipeline = MLPipeline(
    model=RandomForestRegressor(n_estimators=100),
    preprocessor_config={
        "scaler_type": "robust",
        "exclude_from_scaling": ["holiday", "workingday"]
    }
)

# Fit and predict
pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_val)
```

**Benefits:**
- âœ… Prevents data leakage (transformations learned only on train)
- âœ… Single object to serialize for deployment
- âœ… Consistent transformations across train/test
- âœ… Easy to integrate into production systems

## ğŸ“Š MLflow Integration

MLflow is used for:
1. **Experiment Tracking**: All hyperparameters and metrics
2. **Model Registry**: Versioned model storage
3. **Reproducibility**: Full experiment context saved
4. **Comparison**: Compare different model runs

**Example Usage:**
```python
from src.models import ModelTrainer

trainer = ModelTrainer(config, paths)
pipeline = trainer.train_model(
    model_type="xgboost",
    X_train=X_train,
    y_train=y_train,
    X_val=X_val,
    y_val=y_val,
    run_name="xgboost_baseline"
)
```

All hyperparameters, metrics, and models are automatically logged to MLflow.

## ğŸ”„ DVC Integration

DVC is used for:
1. **Data Versioning**: Track raw and processed datasets
2. **Model Versioning**: Track trained models
3. **Remote Storage**: S3 integration for large files

**Commands:**
```bash
# Pull data from remote
dvc pull -r raw

# Track new data
dvc add data/raw/bike_sharing_modified.csv

# Push to remote
dvc push -r raw
```

## ğŸš€ Production-Ready Scripts

The project includes executable scripts:

1. **`src/data/make_dataset.py`**
   - Complete data processing pipeline
   - Can be run standalone: `python src/data/make_dataset.py`
   - Or via Makefile: `make data`

2. **`src/models/train_model.py`**
   - Complete training pipeline
   - Trains multiple models
   - Logs to MLflow
   - Can be run standalone or via Makefile

## ğŸ“¦ Package Structure

The project is structured as a **Python package**:

```python
# Install in editable mode
pip install -e .

# Import modules
from src.config import ConfigLoader, ProjectPaths
from src.data import DataLoader, DataCleaner
from src.models import ModelTrainer, MLPipeline
```

## ğŸ”’ Reproducibility Features

1. **Fixed Random Seeds**: Configured in `config.yaml`
2. **Environment Files**: `environment.yml` and `requirements.txt`
3. **Version Control**: Git for code, DVC for data
4. **Configuration**: All parameters in `config.yaml`
5. **Experiment Tracking**: MLflow tracks every experiment
6. **Makefile**: Standardized commands via `make`

## ğŸ“ Best Practices Implemented

- âœ… **Cookiecutter Structure**: Standardized project layout
- âœ… **OOP Design**: Classes with single responsibility
- âœ… **Scikit-Learn Pipelines**: Production-ready ML workflows
- âœ… **MLflow Integration**: Professional experiment tracking
- âœ… **DVC Integration**: Data and model versioning
- âœ… **Configuration Management**: Centralized parameters
- âœ… **Type Hints**: Better code documentation
- âœ… **Logging**: Comprehensive logging throughout
- âœ… **Error Handling**: Graceful error handling
- âœ… **Documentation**: Comprehensive docstrings

## ğŸ”® Future Enhancements

Potential additions:
- [ ] Unit tests (pytest)
- [ ] Integration tests
- [ ] CI/CD pipeline (GitHub Actions)
- [ ] Docker containerization
- [ ] API deployment (FastAPI)
- [ ] Model monitoring (Evidently AI)
- [ ] Automated hyperparameter tuning (Optuna)

---

**Last Updated:** 2025-01-13  
**Version:** 0.1.0

