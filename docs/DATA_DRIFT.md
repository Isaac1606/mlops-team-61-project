# Data Drift Detection and Performance Monitoring

## üìã Tabla de Contenidos

1. [Quick Start](#quick-start)
2. [Introducci√≥n](#introducci√≥n)
3. [¬øQu√© es Data Drift?](#qu√©-es-data-drift)
4. [Arquitectura del Sistema](#arquitectura-del-sistema)
5. [Componentes Principales](#componentes-principales)
6. [Uso del Sistema](#uso-del-sistema)
7. [Ejemplos Pr√°cticos](#ejemplos-pr√°cticos)
8. [Tests y Validaci√≥n](#tests-y-validaci√≥n)
9. [Mejores Pr√°cticas](#mejores-pr√°cticas)
10. [Soluci√≥n de Problemas](#soluci√≥n-de-problemas)

---

## Quick Start

### Ejecutar Tests de Data Drift

```bash
# Todos los tests de drift
make test-drift

# O con pytest
pytest tests/data_drift/ -v
```

### Uso B√°sico en C√≥digo

```python
from src.models.data_drift import DataDriftDetector, PerformanceMonitor
import pandas as pd

# 1. Inicializar detector con datos de entrenamiento
detector = DataDriftDetector(X_train)

# 2. Detectar drift en datos de producci√≥n
drift_results = detector.detect_drift(X_production)

# 3. Verificar resultados
if drift_results['has_drift']:
    print(f"‚ö†Ô∏è Drift detectado! Score: {drift_results['drift_score']:.3f}")
    print(f"Features afectadas: {drift_results['summary']['drifted_features']}")

# 4. Monitorear performance
baseline_metrics = {'mae': 100.0, 'rmse': 150.0, 'r2': 0.85}
monitor = PerformanceMonitor(baseline_metrics, metric_type='mae')

perf_results = monitor.check_performance(y_production, y_pred_production)
if perf_results['has_degradation']:
    print(f"‚ö†Ô∏è Degradaci√≥n: {perf_results['degradation_score']*100:.1f}%")
```

### Documentaci√≥n Completa

Para m√°s detalles, lee las secciones siguientes o consulta los ejemplos en `tests/data_drift/test_data_drift.py`.

---

## Introducci√≥n

Este proyecto incluye un sistema completo de detecci√≥n de **Data Drift** y monitoreo de **degradaci√≥n de performance** dise√±ado para mantener modelos de Machine Learning en producci√≥n. El sistema permite detectar cambios en la distribuci√≥n de los datos y alertar cuando el rendimiento del modelo cae por debajo de los umbrales aceptables.

### Caracter√≠sticas Principales

- ‚úÖ **Detecci√≥n Estad√≠stica de Drift**: Tests de Kolmogorov-Smirnov, Chi-square y PSI
- ‚úÖ **Monitoreo de Performance**: Detecci√≥n autom√°tica de degradaci√≥n
- ‚úÖ **Generaci√≥n de Datos Sint√©ticos**: Para pruebas y validaci√≥n
- ‚úÖ **Alertas Configurables**: Umbrales personalizables por m√©trica
- ‚úÖ **Soporte Multi-tipo**: Features continuas y categ√≥ricas

---

## ¬øQu√© es Data Drift?

**Data Drift** (o "deriva de datos") ocurre cuando la distribuci√≥n de los datos en producci√≥n difiere significativamente de los datos de entrenamiento. Esto puede deberse a:

- Cambios en el comportamiento del usuario
- Cambios estacionales
- Errores en el pipeline de datos
- Cambios en el entorno operativo
- Evoluci√≥n natural del dominio

### Tipos de Drift

1. **Covariate Shift**: Cambio en la distribuci√≥n de las features (X)
2. **Label Shift**: Cambio en la distribuci√≥n del target (y)
3. **Concept Drift**: Cambio en la relaci√≥n X ‚Üí y

Nuestro sistema detecta principalmente **Covariate Shift** mediante tests estad√≠sticos.

---

## Arquitectura del Sistema

El sistema de detecci√≥n de drift est√° implementado en `src/models/data_drift.py` y consta de dos clases principales:

```
DataDriftDetector
‚îú‚îÄ‚îÄ Detecci√≥n de drift en features
‚îÇ   ‚îú‚îÄ‚îÄ Kolmogorov-Smirnov (features continuas)
‚îÇ   ‚îú‚îÄ‚îÄ Chi-square (features categ√≥ricas)
‚îÇ   ‚îî‚îÄ‚îÄ PSI - Population Stability Index
‚îú‚îÄ‚îÄ Generaci√≥n de datos sint√©ticos
‚îÇ   ‚îú‚îÄ‚îÄ Mean shift drift
‚îÇ   ‚îú‚îÄ‚îÄ Variance shift drift
‚îÇ   ‚îî‚îÄ‚îÄ Distribution shift drift
‚îî‚îÄ‚îÄ Reportes detallados por feature

PerformanceMonitor
‚îú‚îÄ‚îÄ Comparaci√≥n con baseline
‚îú‚îÄ‚îÄ C√°lculo de degradaci√≥n
‚îú‚îÄ‚îÄ Alertas configurables
‚îî‚îÄ‚îÄ Soporte para MAE, RMSE, R¬≤
```

---

## Componentes Principales

### 1. DataDriftDetector

Detecta cambios en la distribuci√≥n de datos entre datos de referencia (entrenamiento) y datos actuales (producci√≥n).

#### Inicializaci√≥n

```python
from src.models.data_drift import DataDriftDetector

# Usar todos los features num√©ricos por defecto
detector = DataDriftDetector(
    reference_data=X_train,
    threshold=0.05,  # P-value threshold para tests estad√≠sticos
    psi_threshold=0.25  # Umbral PSI para drift significativo
)

# O especificar features manualmente
detector = DataDriftDetector(
    reference_data=X_train,
    feature_columns=['feature1', 'feature2', 'feature3'],
    categorical_columns=['category1', 'category2'],
    threshold=0.05,
    psi_threshold=0.25
)
```

#### Par√°metros

- **reference_data** (pd.DataFrame): Datos de referencia (entrenamiento)
- **feature_columns** (List[str], opcional): Features continuas a monitorear
- **categorical_columns** (List[str], opcional): Features categ√≥ricas a monitorear
- **threshold** (float, default=0.05): P-value threshold para tests estad√≠sticos
- **psi_threshold** (float, default=0.25): Umbral PSI para drift significativo

#### Detecci√≥n de Drift

```python
# Detectar drift en datos de producci√≥n
results = detector.detect_drift(
    current_data=X_production,
    return_details=True  # Incluir informaci√≥n detallada por feature
)

# Estructura del resultado
{
    'has_drift': bool,           # ¬øSe detect√≥ drift?
    'drift_score': float,        # Score general de drift (0-1+)
    'feature_drifts': {          # Detalles por feature
        'feature1': {
            'type': 'continuous',
            'has_drift': bool,
            'ks_statistic': float,
            'ks_pvalue': float,
            'psi': float,
            'ref_mean': float,
            'curr_mean': float,
            ...
        },
        ...
    },
    'summary': {
        'total_features': int,
        'drifted_features': int,
        'tests_performed': int
    }
}
```

#### Tests Estad√≠sticos

**Para Features Continuas:**
- **Kolmogorov-Smirnov Test**: Compara distribuciones emp√≠ricas
  - P-value < threshold ‚Üí Drift detectado
- **PSI (Population Stability Index)**:
  - PSI < 0.1: Sin cambio significativo
  - 0.1 ‚â§ PSI < 0.25: Cambio moderado
  - PSI ‚â• 0.25: Cambio significativo (drift)

**Para Features Categ√≥ricas:**
- **Chi-square Test**: Compara distribuciones de categor√≠as
  - P-value < threshold ‚Üí Drift detectado

### 2. PerformanceMonitor

Monitorea la degradaci√≥n del rendimiento del modelo comparando m√©tricas actuales con una baseline.

#### Inicializaci√≥n

```python
from src.models.data_drift import PerformanceMonitor

# M√©tricas baseline (obtenidas en validaci√≥n/entrenamiento)
baseline_metrics = {
    'mae': 100.0,
    'rmse': 150.0,
    'r2': 0.85
}

monitor = PerformanceMonitor(
    baseline_metrics=baseline_metrics,
    performance_threshold=0.2,  # 20% de degradaci√≥n aceptable
    metric_type='mae'  # 'mae', 'rmse', o 'r2'
)
```

#### Par√°metros

- **baseline_metrics** (Dict[str, float]): M√©tricas de referencia
- **performance_threshold** (float, default=0.2): Umbral de degradaci√≥n relativa (20% = 0.2)
- **metric_type** (str): M√©trica principal a monitorear ('mae', 'rmse', 'r2')

#### Monitoreo de Performance

```python
# Evaluar performance actual
perf_results = monitor.check_performance(
    y_true=y_production,
    y_pred=y_pred_production
)

# Estructura del resultado
{
    'has_degradation': bool,     # ¬øHay degradaci√≥n?
    'degradation_score': float,  # Score de degradaci√≥n (puede ser negativo si mejor√≥)
    'current_metrics': {         # M√©tricas actuales
        'mae': float,
        'rmse': float,
        'r2': float
    },
    'baseline_metrics': {...},   # M√©tricas de referencia
    'alert': bool                # ¬øDeber√≠a alertar? (degradaci√≥n > threshold * 1.5)
}
```

---

## Uso del Sistema

### Flujo Completo de Monitoreo

```python
from src.models.data_drift import DataDriftDetector, PerformanceMonitor
from src.models.model_evaluator import ModelEvaluator
from src.config.config_loader import ConfigLoader

# 1. Cargar configuraci√≥n
config = ConfigLoader()

# 2. Inicializar detector con datos de entrenamiento
detector = DataDriftDetector(X_train)

# 3. Detectar drift en datos de producci√≥n
drift_results = detector.detect_drift(X_production)

# 4. Si hay drift, evaluar impacto en performance
if drift_results['has_drift']:
    print(f"‚ö†Ô∏è Drift detectado! Score: {drift_results['drift_score']:.3f}")
    print(f"Features afectadas: {drift_results['summary']['drifted_features']}")
    
    # Obtener predicciones del modelo
    y_pred = model.predict(X_production)
    
    # Monitorear performance
    evaluator = ModelEvaluator(config)
    current_metrics = evaluator.evaluate(y_production, y_pred)
    
    # Comparar con baseline
    baseline_metrics = {
        'mae': 100.0,  # Obtenido durante entrenamiento
        'rmse': 150.0,
        'r2': 0.85
    }
    
    monitor = PerformanceMonitor(
        baseline_metrics=baseline_metrics,
        performance_threshold=0.2,
        metric_type='mae'
    )
    
    perf_results = monitor.check_performance(y_production, y_pred)
    
    if perf_results['has_degradation']:
        print(f"‚ö†Ô∏è Degradaci√≥n detectada: {perf_results['degradation_score']*100:.1f}%")
        
        if perf_results['alert']:
            print("üö® ALERTA: Degradaci√≥n significativa detectada!")
            # Aqu√≠ podr√≠as enviar notificaci√≥n, retrenar modelo, etc.
```

### Integraci√≥n en Pipeline de Producci√≥n

```python
def monitor_production_model(model, X_production, y_production, X_train, baseline_metrics):
    """
    Monitoreo completo para modelo en producci√≥n.
    
    Returns:
        dict: Resultados completos de monitoreo
    """
    # 1. Detectar drift
    detector = DataDriftDetector(X_train)
    drift_results = detector.detect_drift(X_production)
    
    # 2. Obtener predicciones
    y_pred = model.predict(X_production)
    
    # 3. Monitorear performance
    monitor = PerformanceMonitor(
        baseline_metrics=baseline_metrics,
        performance_threshold=0.2,
        metric_type='mae'
    )
    perf_results = monitor.check_performance(y_production, y_pred)
    
    # 4. Combinar resultados
    return {
        'drift_detected': drift_results['has_drift'],
        'drift_score': drift_results['drift_score'],
        'performance_degradation': perf_results['has_degradation'],
        'degradation_score': perf_results['degradation_score'],
        'alert': perf_results['alert'],
        'recommendation': _get_recommendation(drift_results, perf_results)
    }

def _get_recommendation(drift_results, perf_results):
    """Generar recomendaci√≥n basada en resultados."""
    if perf_results['alert']:
        return "Retrenar modelo inmediatamente"
    elif drift_results['has_drift'] and perf_results['has_degradation']:
        return "Monitorear de cerca, considerar retrenamiento"
    elif drift_results['has_drift']:
        return "Drift detectado pero sin impacto en performance a√∫n"
    else:
        return "Sistema funcionando normalmente"
```

---

## Ejemplos Pr√°cticos

### Ejemplo 1: Detecci√≥n B√°sica de Drift

```python
import pandas as pd
from src.models.data_drift import DataDriftDetector

# Datos de entrenamiento
X_train = pd.read_csv('data/processed/bike_sharing_features_train.csv')

# Datos de producci√≥n (nuevos datos)
X_production = pd.read_csv('data/production/latest_batch.csv')

# Inicializar detector
detector = DataDriftDetector(X_train, threshold=0.05)

# Detectar drift
results = detector.detect_drift(X_production)

# Analizar resultados
if results['has_drift']:
    print(f"Drift Score: {results['drift_score']:.3f}")
    print(f"Features con drift: {results['summary']['drifted_features']}")
    
    # Detalles por feature
    for feature, info in results['feature_drifts'].items():
        if info['has_drift']:
            print(f"\n{feature}:")
            print(f"  Type: {info['type']}")
            if info['type'] == 'continuous':
                print(f"  PSI: {info['psi']:.3f}")
                print(f"  Reference mean: {info['ref_mean']:.2f}")
                print(f"  Current mean: {info['curr_mean']:.2f}")
```

### Ejemplo 2: Monitoreo Continuo

```python
from src.models.data_drift import PerformanceMonitor
import json
from datetime import datetime

# Baseline (obtenido durante entrenamiento)
baseline_metrics = {
    'mae': 95.3,
    'rmse': 142.1,
    'r2': 0.87
}

monitor = PerformanceMonitor(
    baseline_metrics=baseline_metrics,
    performance_threshold=0.2,
    metric_type='mae'
)

def daily_performance_check(y_true, y_pred):
    """Funci√≥n para ejecutar diariamente."""
    results = monitor.check_performance(y_true, y_pred)
    
    # Registrar resultados
    log_entry = {
        'timestamp': datetime.now().isoformat(),
        'degradation_detected': results['has_degradation'],
        'degradation_score': results['degradation_score'],
        'current_mae': results['current_metrics']['mae'],
        'baseline_mae': results['baseline_metrics']['mae'],
        'alert': results['alert']
    }
    
    # Guardar log
    with open('logs/performance_monitoring.jsonl', 'a') as f:
        f.write(json.dumps(log_entry) + '\n')
    
    # Enviar alerta si es necesario
    if results['alert']:
        send_alert(log_entry)
    
    return results

# Uso diario
daily_performance_check(y_production, y_pred_production)
```

### Ejemplo 3: Generaci√≥n de Datos Sint√©ticos para Testing

```python
from src.models.data_drift import DataDriftDetector

# Datos de referencia
X_train = pd.read_csv('data/processed/bike_sharing_features_train.csv')

# Inicializar detector
detector = DataDriftDetector(X_train)

# Generar datos con drift simulado
synthetic_drifted = detector.generate_synthetic_drift(
    n_samples=200,
    drift_type="mean_shift",      # 'mean_shift', 'variance_shift', 'distribution_shift'
    drift_magnitude=2.0,          # Magnitud del drift (m√∫ltiplo de std)
    features_to_drift=['temp', 'hum', 'windspeed']  # Features a aplicar drift
)

# Verificar que el drift fue aplicado
results = detector.detect_drift(synthetic_drifted)
print(f"Drift simulado detectado: {results['has_drift']}")
print(f"Score: {results['drift_score']:.3f}")

# Usar para validar sistema de detecci√≥n
```

### Ejemplo 4: Integraci√≥n con Modelo Entrenado

```python
from src.models.model_trainer import ModelTrainer
from src.models.model_evaluator import ModelEvaluator
from src.models.data_drift import DataDriftDetector, PerformanceMonitor
from src.config.config_loader import ConfigLoader
from src.config.paths import ProjectPaths

# Configuraci√≥n
config = ConfigLoader()
paths = ProjectPaths(config)

# 1. Entrenar modelo y obtener baseline
trainer = ModelTrainer(config, paths)
pipeline = trainer.train_model(
    model_type="random_forest",
    X_train=X_train,
    y_train=y_train,
    X_val=X_val,
    y_val=y_val
)

evaluator = ModelEvaluator(config)
y_val_pred = pipeline.predict(X_val)
baseline_metrics = evaluator.evaluate(y_val.values, y_val_pred)

# 2. Configurar monitoreo
detector = DataDriftDetector(X_train)
monitor = PerformanceMonitor(
    baseline_metrics=baseline_metrics,
    performance_threshold=0.2,
    metric_type='mae'
)

# 3. Monitorear en producci√≥n
def production_monitoring(X_prod, y_prod):
    # Detectar drift
    drift_results = detector.detect_drift(X_prod)
    
    # Predecir
    y_pred = pipeline.predict(X_prod)
    
    # Monitorear performance
    perf_results = monitor.check_performance(y_prod.values, y_pred)
    
    return {
        'drift': drift_results,
        'performance': perf_results
    }

# Ejecutar monitoreo
results = production_monitoring(X_production, y_production)
```

---

## Tests y Validaci√≥n

### Ejecutar Tests de Data Drift

```bash
# Todos los tests de drift
make test-drift

# O con pytest directamente
pytest tests/data_drift/ -v

# Tests espec√≠ficos
pytest tests/data_drift/test_data_drift.py::TestDataDriftDetector::test_mean_shift_drift -v
pytest tests/data_drift/test_data_drift.py::TestPerformanceMonitor -v
```

### Tests Disponibles

**DataDriftDetector:**
- `test_no_drift`: Verifica que datos id√©nticos no generan drift
- `test_mean_shift_drift`: Detecta drift por cambio de media
- `test_variance_shift_drift`: Detecta drift por cambio de varianza
- `test_categorical_drift`: Detecta drift en features categ√≥ricas
- `test_generate_synthetic_drift`: Valida generaci√≥n de datos sint√©ticos

**PerformanceMonitor:**
- `test_no_degradation`: Verifica cuando no hay degradaci√≥n
- `test_mae_degradation`: Detecta degradaci√≥n en MAE
- `test_r2_degradation`: Detecta degradaci√≥n en R¬≤
- `test_alert_threshold`: Verifica sistema de alertas

**Integraci√≥n:**
- `test_drift_detection_with_trained_model`: Integraci√≥n completa
- `test_drift_with_synthetic_data`: Testing con datos sint√©ticos
- `test_end_to_end_drift_monitoring`: Flujo completo

### Cobertura de Tests

Los tests cubren:
- ‚úÖ Detecci√≥n de diferentes tipos de drift
- ‚úÖ Generaci√≥n de datos sint√©ticos
- ‚úÖ Monitoreo de performance
- ‚úÖ Integraci√≥n con modelos entrenados
- ‚úÖ Manejo de edge cases

---

## Mejores Pr√°cticas

### 1. Selecci√≥n de Features a Monitorear

- **Monitorear features cr√≠ticas**: Prioriza features con alta importancia
- **Balance**: No monitorees todas las features (ruido), pero incluye las relevantes
- **Features categ√≥ricas**: Especifica manualmente si son importantes para el negocio

```python
# Mejor: Monitorear features importantes
important_features = ['temp', 'hum', 'windspeed', 'hr', 'workingday']
detector = DataDriftDetector(
    X_train,
    feature_columns=important_features
)
```

### 2. Configuraci√≥n de Umbrales

- **threshold (P-value)**: 0.05 es est√°ndar, pero ajusta seg√∫n tu caso
  - M√°s estricto (0.01): Menos falsos positivos, m√°s falsos negativos
  - Menos estricto (0.10): M√°s alertas, pero m√°s sensibilidad
- **psi_threshold**: 0.25 para drift significativo es razonable
- **performance_threshold**: 0.2 (20%) es un buen punto de partida

```python
# Para producci√≥n cr√≠tica (m√°s estricto)
detector = DataDriftDetector(X_train, threshold=0.01, psi_threshold=0.15)

# Para desarrollo (m√°s permisivo)
detector = DataDriftDetector(X_train, threshold=0.10, psi_threshold=0.30)
```

### 3. Frecuencia de Monitoreo

- **Datos en tiempo real**: Monitoreo continuo o cada hora
- **Datos batch**: Monitoreo despu√©s de cada batch
- **Balance costo/beneficio**: M√°s frecuente = m√°s recursos, m√°s temprana detecci√≥n

```python
# Ejemplo: Monitoreo diario
def daily_drift_check():
    latest_data = load_latest_production_data()
    results = detector.detect_drift(latest_data)
    log_results(results)
    return results
```

### 4. Acciones ante Drift Detectado

1. **Drift sin degradaci√≥n**: Monitorear m√°s de cerca
2. **Drift con degradaci√≥n moderada**: Investigar causas, preparar retrenamiento
3. **Degradaci√≥n significativa (alert)**: Retrenar modelo inmediatamente

```python
def handle_drift_results(drift_results, perf_results):
    if perf_results['alert']:
        # Acci√≥n inmediata
        retrain_model()
        send_notification("Model retraining triggered")
    elif perf_results['has_degradation']:
        # Investigar
        investigate_drift_causes(drift_results)
        schedule_retraining()
    elif drift_results['has_drift']:
        # Monitorear
        increase_monitoring_frequency()
```

### 5. Almacenamiento de Resultados

- Guarda resultados hist√≥ricos para an√°lisis de tendencias
- Mant√©n logs de alertas y acciones tomadas
- Usa para an√°lisis post-mortem y mejora continua

```python
import json
from datetime import datetime

def log_monitoring_results(drift_results, perf_results):
    entry = {
        'timestamp': datetime.now().isoformat(),
        'drift_score': drift_results['drift_score'],
        'drift_detected': drift_results['has_drift'],
        'degradation_score': perf_results['degradation_score'],
        'alert_triggered': perf_results['alert']
    }
    
    with open('logs/monitoring_history.jsonl', 'a') as f:
        f.write(json.dumps(entry) + '\n')
```

---

## Soluci√≥n de Problemas

### Problema: Drift detectado constantemente

**Causa posible**: Umbrales muy estrictos o cambios esperados (estacionalidad)

**Soluci√≥n**:
```python
# Ajustar umbrales o excluir features estacionales
detector = DataDriftDetector(
    X_train,
    threshold=0.10,  # Menos estricto
    psi_threshold=0.30
)
```

### Problema: No se detecta drift cuando deber√≠a

**Causa posible**: Umbrales muy permisivos o drift sutil

**Soluci√≥n**:
```python
# Usar umbrales m√°s estrictos
detector = DataDriftDetector(
    X_train,
    threshold=0.01,  # M√°s estricto
    psi_threshold=0.15
)
```

### Problema: Performance degrada sin drift detectado

**Causa posible**: Concept drift (cambio en relaci√≥n X‚Üíy) no detectable por drift de features

**Soluci√≥n**: Monitorear performance directamente (ya implementado en PerformanceMonitor)

### Problema: PSI muy alto pero p-value no significativo

**Causa posible**: PSI es m√°s sensible a cambios peque√±os que tests estad√≠sticos

**Soluci√≥n**: Confiar en PSI para alertas tempranas, usar p-value para confirmaci√≥n

```python
# Usar ambos criterios
has_drift = (ks_pvalue < threshold) or (psi >= psi_threshold)
```

---

## Referencias

- **PSI (Population Stability Index)**: 
  - Thresholds: PSI < 0.1 (stable), 0.1-0.25 (moderate change), ‚â•0.25 (significant change)
- **Kolmogorov-Smirnov Test**: Test no param√©trico para comparar distribuciones
- **Chi-square Test**: Test para comparar distribuciones categ√≥ricas

---

## Pr√≥ximos Pasos

Mejoras futuras posibles:

1. **Concept Drift Detection**: Detectar cambios en relaci√≥n X‚Üíy
2. **Automated Retraining**: Retrenamiento autom√°tico cuando se detecta degradaci√≥n
3. **Dashboard**: Visualizaci√≥n de m√©tricas de drift y performance
4. **Alertas Multi-canal**: Integraci√≥n con Slack, email, etc.
5. **Drift Explanation**: Explicaci√≥n de qu√© features causan el drift

---

## Contacto y Contribuciones

Para preguntas o mejoras, consulta la documentaci√≥n del proyecto o contacta al equipo.

