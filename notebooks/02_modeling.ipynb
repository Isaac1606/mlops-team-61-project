{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BIKE SHARING DEMAND - MODELING\n",
        "\n",
        "Este notebook contiene el entrenamiento y evaluaci√≥n de modelos baseline para predicci√≥n de demanda de bicicletas compartidas.\n",
        "\n",
        "## Objetivos:\n",
        "1. Entrenar 3 modelos baseline (Linear Regression, Random Forest, XGBoost)\n",
        "2. Registrar experimentos con MLflow\n",
        "3. Evaluar con m√©tricas objetivo (MAE < 50, RMSE < 80, R¬≤ > 0.7)\n",
        "4. Analizar feature importance\n",
        "5. Seleccionar mejor modelo para optimizaci√≥n\n",
        "\n",
        "---\n",
        "\n",
        "**Prerequisitos:**\n",
        "- Datasets normalizados en `data/processed/`\n",
        "- Scaler guardado en `models/scaler.pkl`\n",
        "- Notebook anterior ejecutado con feature engineering completo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. SETUP Y CONFIGURACI√ìN\n",
        "\n",
        "Importamos librer√≠as, configuramos MLflow y definimos variables globales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sistema y paths\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualizaci√≥n\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import (\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    r2_score,\n",
        "    mean_absolute_percentage_error\n",
        ")\n",
        "\n",
        "# MLflow\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import mlflow.xgboost\n",
        "\n",
        "# Utilities\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# Configuraci√≥n de plots\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úì Librer√≠as importadas correctamente\")\n",
        "print(f\"Fecha de ejecuci√≥n: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Configuraci√≥n de Paths y Constantes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Directorios\n",
        "PROJECT_ROOT = Path.cwd().parent\n",
        "DATA_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
        "MODELS_DIR = PROJECT_ROOT / 'models'\n",
        "MLFLOW_DIR = PROJECT_ROOT / 'mlruns'\n",
        "\n",
        "# Crear directorios si no existen\n",
        "MLFLOW_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# M√©tricas objetivo (seg√∫n an√°lisis EDA)\n",
        "TARGET_METRICS = {\n",
        "    'MAE': 50,      # Mean Absolute Error < 50\n",
        "    'RMSE': 80,     # Root Mean Squared Error < 80\n",
        "    'R2': 0.7,      # R¬≤ > 0.7\n",
        "    'MAPE': 25      # Mean Absolute Percentage Error < 25%\n",
        "}\n",
        "\n",
        "# Configuraci√≥n MLflow\n",
        "EXPERIMENT_NAME = \"bike-sharing-demand-baseline\"\n",
        "mlflow.set_tracking_uri(f\"file:///{MLFLOW_DIR}\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CONFIGURACI√ìN DEL PROYECTO\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Project Root: {PROJECT_ROOT}\")\n",
        "print(f\"Data Directory: {DATA_DIR}\")\n",
        "print(f\"Models Directory: {MODELS_DIR}\")\n",
        "print(f\"MLflow Tracking: {MLFLOW_DIR}\")\n",
        "print(f\"\\nM√©tricas Objetivo:\")\n",
        "for metric, target in TARGET_METRICS.items():\n",
        "    print(f\"  ‚Ä¢ {metric}: {'<' if metric != 'R2' else '>'} {target}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. CARGA DE DATOS\n",
        "\n",
        "Cargamos los datasets normalizados generados en el notebook anterior.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Cargar Datasets Normalizados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar datasets\n",
        "train_df = pd.read_csv(DATA_DIR / 'bike_sharing_features_train_normalized.csv')\n",
        "val_df = pd.read_csv(DATA_DIR / 'bike_sharing_features_validation_normalized.csv')\n",
        "test_df = pd.read_csv(DATA_DIR / 'bike_sharing_features_test_normalized.csv')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DATASETS CARGADOS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Train: {train_df.shape}\")\n",
        "print(f\"  Fecha inicio: {train_df['timestamp'].min()}\")\n",
        "print(f\"  Fecha fin:    {train_df['timestamp'].max()}\")\n",
        "print(f\"\\nValidation: {val_df.shape}\")\n",
        "print(f\"  Fecha inicio: {val_df['timestamp'].min()}\")\n",
        "print(f\"  Fecha fin:    {val_df['timestamp'].max()}\")\n",
        "print(f\"\\nTest: {test_df.shape}\")\n",
        "print(f\"  Fecha inicio: {test_df['timestamp'].min()}\")\n",
        "print(f\"  Fecha fin:    {test_df['timestamp'].max()}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Verificar integridad\n",
        "assert train_df.shape[1] == val_df.shape[1] == test_df.shape[1], \"Datasets tienen diferente n√∫mero de columnas\"\n",
        "assert train_df.isnull().sum().sum() == 0, \"Train tiene valores nulos\"\n",
        "assert val_df.isnull().sum().sum() == 0, \"Validation tiene valores nulos\"\n",
        "assert test_df.isnull().sum().sum() == 0, \"Test tiene valores nulos\"\n",
        "\n",
        "print(\"\\n‚úì Verificaci√≥n de integridad completada\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Preparar Features y Target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir columnas a excluir (metadata y targets)\n",
        "exclude_cols = ['timestamp', 'dteday', 'cnt', 'casual', 'registered']\n",
        "\n",
        "# Features (todas excepto las excluidas)\n",
        "feature_cols = [col for col in train_df.columns if col not in exclude_cols]\n",
        "\n",
        "# Separar X e y\n",
        "X_train = train_df[feature_cols].values\n",
        "y_train = train_df['cnt'].values\n",
        "\n",
        "X_val = val_df[feature_cols].values\n",
        "y_val = val_df['cnt'].values\n",
        "\n",
        "X_test = test_df[feature_cols].values\n",
        "y_test = test_df['cnt'].values\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FEATURES Y TARGET\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Total features: {len(feature_cols)}\")\n",
        "print(f\"\\nFeatures incluidos (primeros 10):\")\n",
        "for i, feat in enumerate(feature_cols[:10], 1):\n",
        "    print(f\"  {i:2d}. {feat}\")\n",
        "print(f\"  ... y {len(feature_cols) - 10} m√°s\")\n",
        "\n",
        "print(f\"\\nTarget: cnt (demanda total de bicicletas)\")\n",
        "print(f\"\\nShapes:\")\n",
        "print(f\"  X_train: {X_train.shape}\")\n",
        "print(f\"  y_train: {y_train.shape}\")\n",
        "print(f\"  X_val:   {X_val.shape}\")\n",
        "print(f\"  y_val:   {y_val.shape}\")\n",
        "print(f\"  X_test:  {X_test.shape}\")\n",
        "print(f\"  y_test:  {y_test.shape}\")\n",
        "\n",
        "print(f\"\\nEstad√≠sticas del target (cnt):\")\n",
        "print(f\"  Train - Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}, Min: {y_train.min():.0f}, Max: {y_train.max():.0f}\")\n",
        "print(f\"  Val   - Mean: {y_val.mean():.2f}, Std: {y_val.std():.2f}, Min: {y_val.min():.0f}, Max: {y_val.max():.0f}\")\n",
        "print(f\"  Test  - Mean: {y_test.mean():.2f}, Std: {y_test.std():.2f}, Min: {y_test.min():.0f}, Max: {y_test.max():.0f}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. CONFIGURACI√ìN DE MLFLOW\n",
        "\n",
        "Configuramos el experimento de MLflow para tracking de modelos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear o obtener experimento\n",
        "try:\n",
        "    experiment_id = mlflow.create_experiment(\n",
        "        EXPERIMENT_NAME,\n",
        "        tags={\n",
        "            \"project\": \"mlops-team-61\",\n",
        "            \"phase\": \"baseline-models\",\n",
        "            \"dataset\": \"bike-sharing\",\n",
        "            \"features\": str(len(feature_cols))\n",
        "        }\n",
        "    )\n",
        "    print(f\"‚úì Experimento creado: {EXPERIMENT_NAME}\")\n",
        "except:\n",
        "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
        "    experiment_id = experiment.experiment_id\n",
        "    print(f\"‚úì Experimento existente: {EXPERIMENT_NAME}\")\n",
        "\n",
        "mlflow.set_experiment(EXPERIMENT_NAME)\n",
        "\n",
        "print(f\"  Experiment ID: {experiment_id}\")\n",
        "print(f\"  Tracking URI: {mlflow.get_tracking_uri()}\")\n",
        "print(f\"\\nüìä Para ver MLflow UI, ejecutar en terminal:\")\n",
        "print(f\"   mlflow ui --backend-store-uri {mlflow.get_tracking_uri()}\")\n",
        "print(f\"   Luego abrir: http://localhost:5000\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. FUNCIONES DE EVALUACI√ìN\n",
        "\n",
        "Definimos funciones reutilizables para evaluar modelos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(y_true, y_pred, dataset_name=\"Validation\"):\n",
        "    \"\"\"\n",
        "    Eval√∫a un modelo con m√∫ltiples m√©tricas.\n",
        "    \n",
        "    Args:\n",
        "        y_true: Valores reales\n",
        "        y_pred: Valores predichos\n",
        "        dataset_name: Nombre del dataset (Train/Validation/Test)\n",
        "    \n",
        "    Returns:\n",
        "        dict: Diccionario con m√©tricas calculadas\n",
        "    \"\"\"\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
        "    \n",
        "    # M√©tricas adicionales\n",
        "    residuals = y_true - y_pred\n",
        "    \n",
        "    metrics = {\n",
        "        'mae': mae,\n",
        "        'mse': mse,\n",
        "        'rmse': rmse,\n",
        "        'r2': r2,\n",
        "        'mape': mape,\n",
        "        'residuals_mean': residuals.mean(),\n",
        "        'residuals_std': residuals.std()\n",
        "    }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "\n",
        "def print_metrics(metrics, dataset_name=\"Validation\", targets=TARGET_METRICS):\n",
        "    \"\"\"\n",
        "    Imprime m√©tricas en formato legible con comparaci√≥n vs targets.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"M√âTRICAS - {dataset_name.upper()}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # MAE\n",
        "    mae_status = \"‚úì\" if metrics['mae'] < targets['MAE'] else \"‚úó\"\n",
        "    print(f\"MAE:  {metrics['mae']:8.2f}  {mae_status}  (target: < {targets['MAE']})\")\n",
        "    \n",
        "    # RMSE\n",
        "    rmse_status = \"‚úì\" if metrics['rmse'] < targets['RMSE'] else \"‚úó\"\n",
        "    print(f\"RMSE: {metrics['rmse']:8.2f}  {rmse_status}  (target: < {targets['RMSE']})\")\n",
        "    \n",
        "    # R¬≤\n",
        "    r2_status = \"‚úì\" if metrics['r2'] > targets['R2'] else \"‚úó\"\n",
        "    print(f\"R¬≤:   {metrics['r2']:8.4f}  {r2_status}  (target: > {targets['R2']})\")\n",
        "    \n",
        "    # MAPE\n",
        "    mape_status = \"‚úì\" if metrics['mape'] < targets['MAPE'] else \"‚úó\"\n",
        "    print(f\"MAPE: {metrics['mape']:8.2f}% {mape_status}  (target: < {targets['MAPE']}%)\")\n",
        "    \n",
        "    print(f\"\\nResiduos:\")\n",
        "    print(f\"  Mean: {metrics['residuals_mean']:8.2f}  (debe estar ~0)\")\n",
        "    print(f\"  Std:  {metrics['residuals_std']:8.2f}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "\n",
        "def plot_predictions(y_true, y_pred, title=\"Predicciones vs Reales\", sample_size=500):\n",
        "    \"\"\"\n",
        "    Visualiza predicciones vs valores reales.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Scatter plot (muestra)\n",
        "    idx = np.random.choice(len(y_true), min(sample_size, len(y_true)), replace=False)\n",
        "    axes[0].scatter(y_true[idx], y_pred[idx], alpha=0.5, s=20)\n",
        "    axes[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], \n",
        "                 'r--', lw=2, label='Perfect Prediction')\n",
        "    axes[0].set_xlabel('Valores Reales')\n",
        "    axes[0].set_ylabel('Predicciones')\n",
        "    axes[0].set_title(f'{title} - Scatter')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Distribuci√≥n de residuos\n",
        "    residuals = y_true - y_pred\n",
        "    axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
        "    axes[1].axvline(0, color='red', linestyle='--', lw=2, label='Zero Error')\n",
        "    axes[1].axvline(residuals.mean(), color='green', linestyle='--', lw=2, \n",
        "                    label=f'Mean: {residuals.mean():.2f}')\n",
        "    axes[1].set_xlabel('Residuos (Real - Predicci√≥n)')\n",
        "    axes[1].set_ylabel('Frecuencia')\n",
        "    axes[1].set_title('Distribuci√≥n de Residuos')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úì Funciones de evaluaci√≥n definidas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. MODELO 1: LINEAR REGRESSION (RIDGE)\n",
        "\n",
        "Modelo baseline simple con regularizaci√≥n Ridge.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.1 Entrenamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"MODELO 1: RIDGE REGRESSION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Hiperpar√°metros\n",
        "ridge_params = {\n",
        "    'alpha': 1.0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# MLflow Run\n",
        "with mlflow.start_run(run_name=\"ridge_baseline\") as run:\n",
        "    \n",
        "    # Log parameters\n",
        "    mlflow.log_params(ridge_params)\n",
        "    mlflow.log_param(\"model_type\", \"Ridge Regression\")\n",
        "    mlflow.log_param(\"n_features\", len(feature_cols))\n",
        "    \n",
        "    # Entrenar modelo\n",
        "    print(\"\\nEntrenando Ridge Regression...\")\n",
        "    ridge_model = Ridge(**ridge_params)\n",
        "    ridge_model.fit(X_train, y_train)\n",
        "    print(\"‚úì Modelo entrenado\")\n",
        "    \n",
        "    # Predicciones\n",
        "    y_train_pred_ridge = ridge_model.predict(X_train)\n",
        "    y_val_pred_ridge = ridge_model.predict(X_val)\n",
        "    y_test_pred_ridge = ridge_model.predict(X_test)\n",
        "    \n",
        "    # Evaluar\n",
        "    train_metrics_ridge = evaluate_model(y_train, y_train_pred_ridge, \"Train\")\n",
        "    val_metrics_ridge = evaluate_model(y_val, y_val_pred_ridge, \"Validation\")\n",
        "    test_metrics_ridge = evaluate_model(y_test, y_test_pred_ridge, \"Test\")\n",
        "    \n",
        "    # Log metrics\n",
        "    for prefix, metrics in [('train', train_metrics_ridge), \n",
        "                             ('val', val_metrics_ridge),\n",
        "                             ('test', test_metrics_ridge)]:\n",
        "        for metric_name, value in metrics.items():\n",
        "            mlflow.log_metric(f\"{prefix}_{metric_name}\", value)\n",
        "    \n",
        "    # Log model\n",
        "    mlflow.sklearn.log_model(ridge_model, \"model\", \n",
        "                              registered_model_name=\"bike-demand-ridge\")\n",
        "    \n",
        "    # Tags\n",
        "    mlflow.set_tags({\n",
        "        \"model_family\": \"linear\",\n",
        "        \"complexity\": \"low\",\n",
        "        \"regularization\": \"L2\"\n",
        "    })\n",
        "    \n",
        "    print(f\"\\n‚úì Run ID: {run.info.run_id}\")\n",
        "\n",
        "# Mostrar resultados\n",
        "print_metrics(train_metrics_ridge, \"Train\")\n",
        "print_metrics(val_metrics_ridge, \"Validation\")\n",
        "print_metrics(test_metrics_ridge, \"Test\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.2 Visualizaci√≥n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_predictions(y_val, y_val_pred_ridge, \"Ridge Regression - Validation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. MODELO 2: RANDOM FOREST\n",
        "\n",
        "Modelo ensemble basado en √°rboles de decisi√≥n.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.1 Entrenamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"MODELO 2: RANDOM FOREST\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Hiperpar√°metros\n",
        "rf_params = {\n",
        "    'n_estimators': 100,\n",
        "    'max_depth': 20,\n",
        "    'min_samples_split': 5,\n",
        "    'min_samples_leaf': 2,\n",
        "    'max_features': 'sqrt',\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1\n",
        "}\n",
        "\n",
        "# MLflow Run\n",
        "with mlflow.start_run(run_name=\"random_forest_baseline\") as run:\n",
        "    \n",
        "    # Log parameters\n",
        "    mlflow.log_params(rf_params)\n",
        "    mlflow.log_param(\"model_type\", \"Random Forest\")\n",
        "    mlflow.log_param(\"n_features\", len(feature_cols))\n",
        "    \n",
        "    # Entrenar modelo\n",
        "    print(\"\\nEntrenando Random Forest...\")\n",
        "    rf_model = RandomForestRegressor(**rf_params)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    print(\"‚úì Modelo entrenado\")\n",
        "    \n",
        "    # Predicciones\n",
        "    y_train_pred_rf = rf_model.predict(X_train)\n",
        "    y_val_pred_rf = rf_model.predict(X_val)\n",
        "    y_test_pred_rf = rf_model.predict(X_test)\n",
        "    \n",
        "    # Evaluar\n",
        "    train_metrics_rf = evaluate_model(y_train, y_train_pred_rf, \"Train\")\n",
        "    val_metrics_rf = evaluate_model(y_val, y_val_pred_rf, \"Validation\")\n",
        "    test_metrics_rf = evaluate_model(y_test, y_test_pred_rf, \"Test\")\n",
        "    \n",
        "    # Log metrics\n",
        "    for prefix, metrics in [('train', train_metrics_rf), \n",
        "                             ('val', val_metrics_rf),\n",
        "                             ('test', test_metrics_rf)]:\n",
        "        for metric_name, value in metrics.items():\n",
        "            mlflow.log_metric(f\"{prefix}_{metric_name}\", value)\n",
        "    \n",
        "    # Feature importance\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_cols,\n",
        "        'importance': rf_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    # Log feature importance como artifact\n",
        "    importance_path = MODELS_DIR / 'rf_feature_importance.csv'\n",
        "    feature_importance.to_csv(importance_path, index=False)\n",
        "    mlflow.log_artifact(str(importance_path))\n",
        "    \n",
        "    # Log model\n",
        "    mlflow.sklearn.log_model(rf_model, \"model\", \n",
        "                              registered_model_name=\"bike-demand-rf\")\n",
        "    \n",
        "    # Tags\n",
        "    mlflow.set_tags({\n",
        "        \"model_family\": \"ensemble\",\n",
        "        \"complexity\": \"medium\",\n",
        "        \"base_learner\": \"decision_tree\"\n",
        "    })\n",
        "    \n",
        "    print(f\"\\n‚úì Run ID: {run.info.run_id}\")\n",
        "\n",
        "# Mostrar resultados\n",
        "print_metrics(train_metrics_rf, \"Train\")\n",
        "print_metrics(val_metrics_rf, \"Validation\")\n",
        "print_metrics(test_metrics_rf, \"Test\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.2 Feature Importance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mostrar top 20 features\n",
        "print(\"=\"*70)\n",
        "print(\"TOP 20 FEATURES M√ÅS IMPORTANTES - RANDOM FOREST\")\n",
        "print(\"=\"*70)\n",
        "print(feature_importance.head(20).to_string(index=False))\n",
        "\n",
        "# Visualizar\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "top_features = feature_importance.head(20)\n",
        "ax.barh(range(len(top_features)), top_features['importance'])\n",
        "ax.set_yticks(range(len(top_features)))\n",
        "ax.set_yticklabels(top_features['feature'])\n",
        "ax.invert_yaxis()\n",
        "ax.set_xlabel('Importancia')\n",
        "ax.set_title('Top 20 Features - Random Forest')\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.3 Visualizaci√≥n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_predictions(y_val, y_val_pred_rf, \"Random Forest - Validation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. MODELO 3: XGBOOST\n",
        "\n",
        "Modelo de gradient boosting (modelo principal seg√∫n ML Canvas).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.1 Entrenamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"MODELO 3: XGBOOST\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Hiperpar√°metros\n",
        "xgb_params = {\n",
        "    'n_estimators': 100,\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.1,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'min_child_weight': 3,\n",
        "    'gamma': 0,\n",
        "    'reg_alpha': 0,\n",
        "    'reg_lambda': 1,\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1\n",
        "}\n",
        "\n",
        "# MLflow Run\n",
        "with mlflow.start_run(run_name=\"xgboost_baseline\") as run:\n",
        "    \n",
        "    # Log parameters\n",
        "    mlflow.log_params(xgb_params)\n",
        "    mlflow.log_param(\"model_type\", \"XGBoost\")\n",
        "    mlflow.log_param(\"n_features\", len(feature_cols))\n",
        "    \n",
        "    # Entrenar modelo\n",
        "    print(\"\\nEntrenando XGBoost...\")\n",
        "    xgb_model = XGBRegressor(**xgb_params)\n",
        "    xgb_model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_train, y_train), (X_val, y_val)],\n",
        "        verbose=False\n",
        "    )\n",
        "    print(\"‚úì Modelo entrenado\")\n",
        "    \n",
        "    # Predicciones\n",
        "    y_train_pred_xgb = xgb_model.predict(X_train)\n",
        "    y_val_pred_xgb = xgb_model.predict(X_val)\n",
        "    y_test_pred_xgb = xgb_model.predict(X_test)\n",
        "    \n",
        "    # Evaluar\n",
        "    train_metrics_xgb = evaluate_model(y_train, y_train_pred_xgb, \"Train\")\n",
        "    val_metrics_xgb = evaluate_model(y_val, y_val_pred_xgb, \"Validation\")\n",
        "    test_metrics_xgb = evaluate_model(y_test, y_test_pred_xgb, \"Test\")\n",
        "    \n",
        "    # Log metrics\n",
        "    for prefix, metrics in [('train', train_metrics_xgb), \n",
        "                             ('val', val_metrics_xgb),\n",
        "                             ('test', test_metrics_xgb)]:\n",
        "        for metric_name, value in metrics.items():\n",
        "            mlflow.log_metric(f\"{prefix}_{metric_name}\", value)\n",
        "    \n",
        "    # Feature importance\n",
        "    feature_importance_xgb = pd.DataFrame({\n",
        "        'feature': feature_cols,\n",
        "        'importance': xgb_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    # Log feature importance\n",
        "    importance_path_xgb = MODELS_DIR / 'xgb_feature_importance.csv'\n",
        "    feature_importance_xgb.to_csv(importance_path_xgb, index=False)\n",
        "    mlflow.log_artifact(str(importance_path_xgb))\n",
        "    \n",
        "    # Log model\n",
        "    mlflow.xgboost.log_model(xgb_model, \"model\", \n",
        "                              registered_model_name=\"bike-demand-xgboost\")\n",
        "    \n",
        "    # Tags\n",
        "    mlflow.set_tags({\n",
        "        \"model_family\": \"boosting\",\n",
        "        \"complexity\": \"medium\",\n",
        "        \"algorithm\": \"gradient_boosting\"\n",
        "    })\n",
        "    \n",
        "    print(f\"\\n‚úì Run ID: {run.info.run_id}\")\n",
        "\n",
        "# Mostrar resultados\n",
        "print_metrics(train_metrics_xgb, \"Train\")\n",
        "print_metrics(val_metrics_xgb, \"Validation\")\n",
        "print_metrics(test_metrics_xgb, \"Test\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.2 Feature Importance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mostrar top 20 features\n",
        "print(\"=\"*70)\n",
        "print(\"TOP 20 FEATURES M√ÅS IMPORTANTES - XGBOOST\")\n",
        "print(\"=\"*70)\n",
        "print(feature_importance_xgb.head(20).to_string(index=False))\n",
        "\n",
        "# Visualizar\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "top_features_xgb = feature_importance_xgb.head(20)\n",
        "ax.barh(range(len(top_features_xgb)), top_features_xgb['importance'])\n",
        "ax.set_yticks(range(len(top_features_xgb)))\n",
        "ax.set_yticklabels(top_features_xgb['feature'])\n",
        "ax.invert_yaxis()\n",
        "ax.set_xlabel('Importancia')\n",
        "ax.set_title('Top 20 Features - XGBoost')\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.3 Visualizaci√≥n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_predictions(y_val, y_val_pred_xgb, \"XGBoost - Validation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. COMPARACI√ìN DE MODELOS\n",
        "\n",
        "Comparamos los 3 modelos baseline para seleccionar el mejor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear tabla comparativa\n",
        "comparison_df = pd.DataFrame([\n",
        "    {\n",
        "        'Model': 'Ridge Regression',\n",
        "        'Train_MAE': train_metrics_ridge['mae'],\n",
        "        'Val_MAE': val_metrics_ridge['mae'],\n",
        "        'Test_MAE': test_metrics_ridge['mae'],\n",
        "        'Train_RMSE': train_metrics_ridge['rmse'],\n",
        "        'Val_RMSE': val_metrics_ridge['rmse'],\n",
        "        'Test_RMSE': test_metrics_ridge['rmse'],\n",
        "        'Train_R2': train_metrics_ridge['r2'],\n",
        "        'Val_R2': val_metrics_ridge['r2'],\n",
        "        'Test_R2': test_metrics_ridge['r2'],\n",
        "        'Val_MAPE': val_metrics_ridge['mape']\n",
        "    },\n",
        "    {\n",
        "        'Model': 'Random Forest',\n",
        "        'Train_MAE': train_metrics_rf['mae'],\n",
        "        'Val_MAE': val_metrics_rf['mae'],\n",
        "        'Test_MAE': test_metrics_rf['mae'],\n",
        "        'Train_RMSE': train_metrics_rf['rmse'],\n",
        "        'Val_RMSE': val_metrics_rf['rmse'],\n",
        "        'Test_RMSE': test_metrics_rf['rmse'],\n",
        "        'Train_R2': train_metrics_rf['r2'],\n",
        "        'Val_R2': val_metrics_rf['r2'],\n",
        "        'Test_R2': test_metrics_rf['r2'],\n",
        "        'Val_MAPE': val_metrics_rf['mape']\n",
        "    },\n",
        "    {\n",
        "        'Model': 'XGBoost',\n",
        "        'Train_MAE': train_metrics_xgb['mae'],\n",
        "        'Val_MAE': val_metrics_xgb['mae'],\n",
        "        'Test_MAE': test_metrics_xgb['mae'],\n",
        "        'Train_RMSE': train_metrics_xgb['rmse'],\n",
        "        'Val_RMSE': val_metrics_xgb['rmse'],\n",
        "        'Test_RMSE': test_metrics_xgb['rmse'],\n",
        "        'Train_R2': train_metrics_xgb['r2'],\n",
        "        'Val_R2': val_metrics_xgb['r2'],\n",
        "        'Test_R2': test_metrics_xgb['r2'],\n",
        "        'Val_MAPE': val_metrics_xgb['mape']\n",
        "    }\n",
        "])\n",
        "\n",
        "print(\"=\"*100)\n",
        "print(\"COMPARACI√ìN DE MODELOS BASELINE\")\n",
        "print(\"=\"*100)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Identificar mejor modelo por validation\n",
        "best_idx = comparison_df['Val_RMSE'].idxmin()\n",
        "best_model = comparison_df.iloc[best_idx]['Model']\n",
        "\n",
        "print(f\"\\n{'='*100}\")\n",
        "print(f\"üèÜ MEJOR MODELO: {best_model}\")\n",
        "print(f\"{'='*100}\")\n",
        "print(f\"  Validation MAE:  {comparison_df.iloc[best_idx]['Val_MAE']:.2f}  (target: < {TARGET_METRICS['MAE']})\")\n",
        "print(f\"  Validation RMSE: {comparison_df.iloc[best_idx]['Val_RMSE']:.2f}  (target: < {TARGET_METRICS['RMSE']})\")\n",
        "print(f\"  Validation R¬≤:   {comparison_df.iloc[best_idx]['Val_R2']:.4f}  (target: > {TARGET_METRICS['R2']})\")\n",
        "print(f\"  Validation MAPE: {comparison_df.iloc[best_idx]['Val_MAPE']:.2f}%  (target: < {TARGET_METRICS['MAPE']}%)\")\n",
        "\n",
        "# Guardar comparaci√≥n\n",
        "comparison_path = MODELS_DIR / 'model_comparison.csv'\n",
        "comparison_df.to_csv(comparison_path, index=False)\n",
        "print(f\"\\n‚úì Comparaci√≥n guardada en: {comparison_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9. RESUMEN Y PR√ìXIMOS PASOS\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Modelos Entrenados\n",
        "\n",
        "1. **Ridge Regression** - Baseline lineal simple\n",
        "2. **Random Forest** - Ensemble de √°rboles\n",
        "3. **XGBoost** - Gradient boosting (modelo principal)\n",
        "\n",
        "## üìä M√©tricas Objetivo\n",
        "\n",
        "| M√©trica | Target | Descripci√≥n |\n",
        "|---------|--------|-------------|\n",
        "| MAE | < 50 | Error absoluto medio |\n",
        "| RMSE | < 80 | Error cuadr√°tico medio |\n",
        "| R¬≤ | > 0.7 | Coeficiente de determinaci√≥n |\n",
        "| MAPE | < 25% | Error porcentual medio |\n",
        "\n",
        "## üöÄ Pr√≥ximos Pasos\n",
        "\n",
        "1. **An√°lisis de errores por segmentos** (hora, clima, season)\n",
        "2. **Hyperparameter tuning** del mejor modelo\n",
        "3. **Feature selection** basado en importance\n",
        "4. **Ensemble de modelos** (stacking/blending)\n",
        "5. **Evaluaci√≥n exhaustiva** con test set\n",
        "6. **Deployment** del modelo final\n",
        "\n",
        "## üìù Archivos Generados\n",
        "\n",
        "- `models/rf_feature_importance.csv`\n",
        "- `models/xgb_feature_importance.csv`\n",
        "- `models/model_comparison.csv`\n",
        "- MLflow runs en `mlruns/`\n",
        "\n",
        "## üîó MLflow UI\n",
        "\n",
        "Para visualizar experimentos:\n",
        "```bash\n",
        "mlflow ui\n",
        "```\n",
        "\n",
        "Abrir: http://localhost:5000\n",
        "\n",
        "---\n",
        "\n",
        "**Notebook completado:** `02_modeling.ipynb`  \n",
        "**Estado:** ‚úÖ Modelos baseline entrenados  \n",
        "**Siguiente:** Optimizaci√≥n y evaluaci√≥n exhaustiva\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
