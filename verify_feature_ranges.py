#!/usr/bin/env python
"""
Script mejorado para verificar rangos de features con detecci√≥n de outliers.
"""

import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).resolve().parent
sys.path.insert(0, str(project_root))

import pandas as pd
import numpy as np
from src.config import ConfigLoader, ProjectPaths
from src.data import DataLoader

def safe_convert_to_numeric(series):
    """Convierte una serie a num√©rico de forma segura."""
    # Primero intenta convertir directamente
    try:
        return pd.to_numeric(series, errors='coerce')
    except Exception:
        # Si falla, intenta limpiar strings vac√≠os y espacios
        cleaned = series.replace(['', ' ', '\n', '\t'], np.nan)
        return pd.to_numeric(cleaned, errors='coerce')

def detect_outliers(values, feature_name, z_threshold=3):
    """Detecta outliers usando Z-score."""
    if len(values) < 3:
        return pd.Series([False] * len(values), index=values.index)
    
    z_scores = np.abs((values - values.mean()) / values.std())
    outliers = z_scores > z_threshold
    return outliers

def analyze_feature_ranges(df: pd.DataFrame, dataset_name: str, features: list):
    """Analiza los rangos y estad√≠sticas de features espec√≠ficas con detecci√≥n de outliers."""
    print(f"\n{'='*70}")
    print(f"AN√ÅLISIS: {dataset_name}")
    print(f"{'='*70}")
    print(f"Total filas: {len(df)}")
    print(f"Total columnas: {len(df.columns)}")
    
    print(f"\n{'Feature':<20} {'Min':<12} {'Max':<12} {'Mean':<12} {'Std':<12} {'Rango':<20} {'Estado':<20} {'Outliers':<15}")
    print("-" * 125)
    
    results = {}
    for feat in features:
        if feat in df.columns:
            # Convertir a num√©rico de forma segura
            values_series = safe_convert_to_numeric(df[feat])
            values = values_series.dropna()
            
            if len(values) > 0:
                # Detectar outliers
                outliers = detect_outliers(values, feat, z_threshold=3)
                n_outliers = outliers.sum()
                
                # Calcular estad√≠sticas SIN outliers
                values_clean = values[~outliers] if n_outliers > 0 else values
                
                min_val = float(values.min())
                max_val = float(values.max())
                mean_val = float(values.mean())
                std_val = float(values.std())
                
                # Estad√≠sticas sin outliers
                if len(values_clean) > 0:
                    min_clean = float(values_clean.min())
                    max_clean = float(values_clean.max())
                    mean_clean = float(values_clean.mean())
                else:
                    min_clean = min_val
                    max_clean = max_val
                    mean_clean = mean_val
                
                range_str = f"[{min_val:.4f}, {max_val:.4f}]"
                
                # Determinar si est√° normalizado (0-1) usando valores limpios
                is_normalized = (min_clean >= -0.1 and max_clean <= 1.1)
                
                status = '‚úÖ Normalizado' if is_normalized else '‚ùå NO normalizado'
                
                outlier_info = f"{n_outliers} ({n_outliers/len(values)*100:.1f}%)" if n_outliers > 0 else "0"
                
                print(f"{feat:<20} {min_val:<12.4f} {max_val:<12.4f} {mean_val:<12.4f} {std_val:<12.4f} {range_str:<20} {status:<20} {outlier_info:<15}")
                
                # Mostrar detalles de outliers si existen
                if n_outliers > 0:
                    outlier_values = values[outliers].sort_values(ascending=False)
                    print(f"   ‚ö†Ô∏è  Outliers detectados (top 5): {list(outlier_values.head(5).values)}")
                    print(f"   üìä Rango SIN outliers: [{min_clean:.4f}, {max_clean:.4f}]")
                
                results[feat] = {
                    'min': min_val,
                    'max': max_val,
                    'mean': mean_val,
                    'std': std_val,
                    'min_clean': min_clean,
                    'max_clean': max_clean,
                    'mean_clean': mean_clean,
                    'is_normalized': is_normalized,
                    'count': len(values),
                    'nulls': values_series.isnull().sum(),
                    'outliers': n_outliers,
                    'dtype': str(df[feat].dtype)
                }
            else:
                print(f"{feat:<20} {'N/A':<12} {'N/A':<12} {'N/A':<12} {'N/A':<12} {'Sin datos':<20} {'‚ö†Ô∏è Todos NaN':<20} {'N/A':<15}")
        else:
            print(f"{feat:<20} {'NO EXISTE':<12}")
            # Mostrar columnas disponibles similares
            similar = [col for col in df.columns if feat.lower() in col.lower() or col.lower() in feat.lower()]
            if similar:
                print(f"   (Columnas similares encontradas: {', '.join(similar[:3])})")
    
    return results

def load_raw_data_safe(data_loader):
    """Carga datos raw de forma segura con manejo de errores."""
    try:
        # Intentar cargar directamente
        df = data_loader.load_raw_data()
        return df
    except Exception as e:
        print(f"‚ö†Ô∏è Error al cargar con DataLoader: {e}")
        print("   Intentando cargar directamente desde archivo...")
        
        try:
            # Cargar directamente desde el archivo
            config = ConfigLoader()
            paths = ProjectPaths(config)
            raw_file = paths.raw_data_file
            
            if not raw_file.exists():
                raise FileNotFoundError(f"Archivo no encontrado: {raw_file}")
            
            # Cargar con opciones m√°s permisivas
            df = pd.read_csv(
                raw_file,
                low_memory=False,
                na_values=['', ' ', 'NA', 'N/A', 'null', 'NULL', 'None'],
                keep_default_na=True
            )
            
            print(f"‚úÖ Datos cargados directamente: {len(df)} filas, {len(df.columns)} columnas")
            
            # Mostrar tipos de datos
            print("\nüìä Tipos de datos en columnas relevantes:")
            for feat in ['temp', 'atemp', 'hum', 'windspeed']:
                if feat in df.columns:
                    sample_val = df[feat].iloc[0] if len(df) > 0 else 'N/A'
                    print(f"   {feat}: {df[feat].dtype} (ejemplo: {sample_val})")
            
            return df
        except Exception as e2:
            print(f"‚ùå Error tambi√©n al cargar directamente: {e2}")
            raise

def main():
    """Funci√≥n principal."""
    print("="*70)
    print("VERIFICACI√ìN DE RANGOS DE FEATURES (CON DETECCI√ìN DE OUTLIERS)")
    print("="*70)
    print("\nEste script verifica los rangos originales de las features")
    print("y detecta valores an√≥malos (outliers) que pueden distorsionar los rangos.\n")
    
    # Configuraci√≥n
    config = ConfigLoader()
    paths = ProjectPaths(config)
    data_loader = DataLoader(paths)
    
    # Features a verificar
    features_to_check = ['temp', 'atemp', 'hum', 'windspeed']
    
    # ====================================================================
    # 1. AN√ÅLISIS DE DATOS RAW (originales, sin procesar)
    # ====================================================================
    raw_results = {}
    try:
        print("\n" + "="*70)
        print("1. CARGANDO DATOS RAW (originales)")
        print("="*70)
        df_raw = load_raw_data_safe(data_loader)
        
        raw_results = analyze_feature_ranges(df_raw, "DATOS RAW", features_to_check)
        
    except FileNotFoundError as e:
        print(f"‚ùå Error: {e}")
        print("   Saltando an√°lisis de datos raw...")
    except Exception as e:
        print(f"‚ùå Error inesperado cargando datos raw: {e}")
        import traceback
        print("\n   Detalles del error:")
        traceback.print_exc()
        print("\n   Continuando con datos processed...")
    
    # ====================================================================
    # 2. AN√ÅLISIS DE DATOS PROCESSED (despu√©s de feature engineering)
    # ====================================================================
    processed_results = {}
    try:
        print("\n" + "="*70)
        print("2. CARGANDO DATOS PROCESSED (despu√©s de feature engineering)")
        print("="*70)
        df_train = data_loader.load_processed_data("train", normalized=False)
        
        processed_results = analyze_feature_ranges(df_train, "DATOS PROCESSED (TRAIN)", features_to_check)
        
    except FileNotFoundError as e:
        print(f"‚ùå Error: {e}")
        print("   Saltando an√°lisis de datos processed...")
        print("   üí° Ejecuta primero: python src/data/make_dataset.py")
    except Exception as e:
        print(f"‚ùå Error inesperado cargando datos processed: {e}")
        import traceback
        print("\n   Detalles del error:")
        traceback.print_exc()
    
    # ====================================================================
    # 3. RESUMEN Y RECOMENDACIONES
    # ====================================================================
    print("\n" + "="*70)
    print("3. RESUMEN Y RECOMENDACIONES")
    print("="*70)
    
    if raw_results and processed_results:
        print("\nüìä Comparaci√≥n RAW vs PROCESSED (SIN outliers):")
        print(f"{'Feature':<20} {'RAW Rango (limpio)':<25} {'PROCESSED Rango (limpio)':<25} {'Cambio':<20}")
        print("-" * 90)
        
        for feat in features_to_check:
            if feat in raw_results and feat in processed_results:
                raw_range = f"[{raw_results[feat]['min_clean']:.3f}, {raw_results[feat]['max_clean']:.3f}]"
                proc_range = f"[{processed_results[feat]['min_clean']:.3f}, {processed_results[feat]['max_clean']:.3f}]"
                
                # Verificar si cambi√≥
                if (abs(raw_results[feat]['min_clean'] - processed_results[feat]['min_clean']) < 0.001 and
                    abs(raw_results[feat]['max_clean'] - processed_results[feat]['max_clean']) < 0.001):
                    cambio = "‚úÖ Sin cambio"
                else:
                    cambio = "‚ö†Ô∏è Cambi√≥"
                
                print(f"{feat:<20} {raw_range:<25} {proc_range:<25} {cambio:<20}")
    
    # Determinar si los datos est√°n normalizados
    print("\nüìã CONCLUSI√ìN:")
    print("-" * 70)
    
    # Priorizar processed_results si est√° disponible, sino usar raw_results
    results_to_use = processed_results if processed_results else raw_results
    
    if results_to_use:
        all_normalized = all(
            results_to_use[feat]['is_normalized'] 
            for feat in features_to_check 
            if feat in results_to_use
        )
        
        dataset_type = "PROCESSED" if processed_results else "RAW"
        
        if all_normalized:
            print(f"‚úÖ Los datos {dataset_type} est√°n NORMALIZADOS (rango 0-1) despu√©s de filtrar outliers")
            print("\n   Esto significa que:")
            print("   - Los datos originales ya vienen normalizados, O")
            print("   - Se normalizaron durante el feature engineering")
            print("\n   üìù RECOMENDACI√ìN:")
            print("   - La API actual (que espera valores 0-1) es CORRECTA")
            print("   - Documenta claramente que los valores deben estar en rango [0, 1]")
            print("   - El preprocessor aplicar√° RobustScaler adicional (consistente con entrenamiento)")
        else:
            print(f"‚ùå Los datos {dataset_type} NO est√°n normalizados (rango fuera de 0-1)")
            print("\n   Esto significa que:")
            print("   - Los datos est√°n en escala original (ej: temp en Celsius)")
            print("   - El preprocessor los normalizar√° autom√°ticamente")
            print("\n   üìù RECOMENDACI√ìN:")
            print("   - La API deber√≠a aceptar valores ORIGINALES (no normalizados)")
            print("   - El usuario NO deber√≠a tener que normalizar manualmente")
            print("   - Actualiza API_EXAMPLES.md con rangos originales")
            
            # Mostrar rangos esperados (sin outliers)
            print("\n   üìä Rangos esperados en la API (sin outliers):")
            for feat in features_to_check:
                if feat in results_to_use:
                    r = results_to_use[feat]
                    print(f"   - {feat}: [{r['min_clean']:.2f}, {r['max_clean']:.2f}]")
    else:
        print("‚ö†Ô∏è No se pudieron cargar los datos para an√°lisis")
        print("   Verifica que los archivos existan y sean accesibles")
    
    # Informaci√≥n sobre outliers
    if raw_results:
        print("\n" + "="*70)
        print("4. INFORMACI√ìN SOBRE OUTLIERS")
        print("="*70)
        total_outliers = sum(raw_results[feat].get('outliers', 0) for feat in features_to_check if feat in raw_results)
        if total_outliers > 0:
            print(f"\n‚ö†Ô∏è Se detectaron {total_outliers} valores an√≥malos (outliers) en los datos RAW.")
            print("   Estos valores son filtrados autom√°ticamente por DataCleaner._validate_and_filter_ranges()")
            print("   durante el procesamiento (desde la versi√≥n actualizada).")
            print("   Los rangos reportados arriba (SIN outliers) son los que realmente se usan.")
        else:
            print("\n‚úÖ No se detectaron outliers significativos en los datos RAW.")
    
    # Informaci√≥n adicional sobre el preprocessor
    print("\n" + "="*70)
    print("5. INFORMACI√ìN SOBRE EL PREPROCESSOR")
    print("="*70)
    print("\nEl pipeline incluye un preprocessor que usa RobustScaler.")
    print("Esto significa que:")
    print("  - Si los datos ya est√°n en 0-1: RobustScaler los transformar√°")
    print("    (pero ser√° consistente con el entrenamiento)")
    print("  - Si los datos est√°n en escala original: RobustScaler los normalizar√°")
    print("    autom√°ticamente usando estad√≠sticas del entrenamiento")
    print("\n‚úÖ En ambos casos, el preprocessor maneja la normalizaci√≥n correctamente.")
    
    print("\n" + "="*70)
    print("VERIFICACI√ìN COMPLETA")
    print("="*70)

if __name__ == "__main__":
    main()